#!/usr/bin/env bash
#SBATCH --job-name=cxr-ldm
#SBATCH --partition=bigbatch
#SBATCH --time=72:00:00
#SBATCH --output=logs/%x-%j.out
#SBATCH --error=logs/%x-%j.err

set -euo pipefail

# --- Pretty printing helpers ---
CYN=$(printf '\033[36m'); BLU=$(printf '\033[34m'); BLD=$(printf '\033[1m');
RST=$(printf '\033[0m')
banner(){ printf "\n${BLU}${BLD}== %s ==${RST}\n" "$*"; }
kv(){ printf "  ${CYN}%-22s${RST} %s\n" "$1" "$2";
}

# ----------------------------
# Defaults (override via env)
# ----------------------------
WORKDIR="${WORKDIR:-$PWD}"
PYFILE="${PYFILE:-run/ldm.py}"
DATA_ROOT="/datasets/mmolefe/cleaned"

# Data & Debug
TASK="${TASK:-TB}"
SPLIT="${SPLIT:-train}"
IMG_SIZE="${IMG_SIZE:-256}"
DISEASE="${DISEASE:-1}" # Use 1 for diseased, 0 for normal
OVERFIT_ONE="${OVERFIT_ONE:-0}"
OVERFIT_K="${OVERFIT_K:-0}"
REPEAT_LEN="${REPEAT_LEN:-500}"

# Autoencoder paths (REQUIRED to be set by launcher script)
AE_CKPT_PATH="${AE_CKPT_PATH:?Must set AE_CKPT_PATH}"
AE_CONFIG_PATH="${AE_CONFIG_PATH:?Must set AE_CONFIG_PATH}"

# LDM Arch
LDM_BASE_CH="${LDM_BASE_CH:-128}"
LDM_CH_MULTS="${LDM_CH_MULTS:-1,2,4}"
LDM_NUM_RES_BLOCKS="${LDM_NUM_RES_BLOCKS:-2}"
LDM_ATTN_RES="${LDM_ATTN_RES:-16}"

# Training
LR="${LR:-1e-4}"
WEIGHT_DECAY="${WEIGHT_DECAY:-1e-4}"
GRAD_CLIP="${GRAD_CLIP:-1.0}"
EPOCHS="${EPOCHS:-500}"
BATCH_PER_DEVICE="${BATCH_PER_DEVICE:-4}"
SAMPLE_EVERY="${SAMPLE_EVERY:-5}"
SEED="${SEED:-42}"

# Experiment naming / output
OUTPUT_ROOT="${OUTPUT_ROOT:-runs_ldm}"
EXP_NAME="${EXP_NAME:-cxr_ldm}"
RUN_NAME="${RUN_NAME:-}"
RESUME_DIR="${RESUME_DIR:-}"

# WandB
WANDB="${WANDB:-1}"
WANDB_PROJECT="${WANDB_PROJECT:-cxr-ldm}"
WANDB_TAGS="${WANDB_TAGS:-slurm}"

# ----------------------------
# Environment & paths
# ----------------------------
mkdir -p "$WORKDIR/logs"
cd "$WORKDIR"
export PYTHONPATH="$WORKDIR${PYTHONPATH:+:$PYTHONPATH}"
export JAX_PLATFORMS=cuda
export XLA_PYTHON_CLIENT_PREALLOCATE=false
export TF_CPP_MIN_LOG_LEVEL=3
source ~/.bashrc
mamba activate "${ENV_NAME:?Must set ENV_NAME}"

# ----------------------------
# Normalize list-like vars
# ----------------------------
LDM_CH_MULTS="${LDM_CH_MULTS//:/,}"
LDM_ATTN_RES="${LDM_ATTN_RES//:/,}"
WANDB_TAGS="${WANDB_TAGS//:/,}"

# ----------------------------
# Build args array safely
# ----------------------------
ARGS=(
  --data_root "$DATA_ROOT" --task "$TASK" --split "$SPLIT" --img_size "$IMG_SIZE"
  --class_filter "$DISEASE"
  --ae_ckpt_path "$AE_CKPT_PATH" --ae_config_path "$AE_CONFIG_PATH"
  --ldm_base_ch "$LDM_BASE_CH" --ldm_ch_mults "$LDM_CH_MULTS" --ldm_num_res_blocks "$LDM_NUM_RES_BLOCKS" --ldm_attn_res "$LDM_ATTN_RES"
  --lr "$LR" --weight_decay "$WEIGHT_DECAY" --grad_clip "$GRAD_CLIP"
  --epochs "$EPOCHS" --batch_per_device "$BATCH_PER_DEVICE"
  --seed "$SEED" --sample_every "$SAMPLE_EVERY"
  --output_root "$OUTPUT_ROOT" --exp_name "$EXP_NAME"
)

# Debugging
if [[ "$OVERFIT_ONE" == "1" ]]; then ARGS+=( --overfit_one --repeat_len "$REPEAT_LEN" );
elif (( OVERFIT_K > 0 )); then ARGS+=( --overfit_k "$OVERFIT_K" );
fi

# Optional run name / resume
[[ -n "$RUN_NAME" ]] && ARGS+=( --run_name "$RUN_NAME" )
[[ -n "$RESUME_DIR" ]] && ARGS+=( --resume_dir "$RESUME_DIR" )

# WandB
if [[ "$WANDB" == "1" ]];
then
  ARGS+=( --wandb --wandb_project "$WANDB_PROJECT" )
  [[ -n "$WANDB_TAGS" ]] && ARGS+=( --wandb_tags "$WANDB_TAGS" )
fi

# ----------------------------
# Info & Run
# ----------------------------
banner "SLURM Job Info"
kv "JOB_ID"         "${SLURM_JOB_ID:-<none>}"
kv "Node"           "$(hostname)"

banner "LDM Training Info"
kv "PYFILE"         "$PYFILE"
kv "TASK"           "$TASK"
kv "DATASET"        "$([[ "$DISEASE" == 1 ]] && echo "Diseased" || echo "Normal")"
kv "EPOCHS"         "$EPOCHS"
kv "BATCHxDEVS"     "$BATCH_PER_DEVICE x $(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | wc -l || echo 1)"
kv "AE_CHECKPOINT"  "$AE_CKPT_PATH"
kv "OVERFIT"        "(of1=$OVERFIT_ONE, tinyK=$OVERFIT_K)"
kv "W&B"            "$WANDB (${WANDB_PROJECT}) tags=[$WANDB_TAGS]"

banner "Run"
srun python -u "$PYFILE" "${ARGS[@]}"